{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mvn, norm, gaussian_kde\n",
    "from scipy.optimize import fsolve\n",
    "# import seaborn as sns\n",
    "from scipy import integrate\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def simulation_v2(time_para, delay, period):\n",
    "    start = time.time()\n",
    "\n",
    "    # Read Portfolio and correlation matrix\n",
    "    path = r'C:\\Users\\Javier\\Documents\\MEGA\\Universitattt\\Master\\Thesis\\CDS_data\\factor_model'\n",
    "    directory = 'time_' + str(time_para) + '_delay_' + str(delay)\n",
    "    file = 'period_' + str(period) + '_portfolio.csv'\n",
    "\n",
    "    Portfolio = pd.read_csv(os.path.join(path,directory,file), sep=',')\n",
    "    # Read the correlation matrix from .csv\n",
    "    Omega = pd.read_csv(r'C:\\Users\\Javier\\Documents\\MEGA\\Universitattt\\Master\\Thesis\\CDS_data\\factor_model\\Ioannis\\Omega.csv', index_col = 0,header = 0)\n",
    "\n",
    "    m = len(Portfolio)    # number of counterparties in the portfolio\n",
    "    N = 10**6             # number of simulations\n",
    "    p = Omega.shape[0]    # number of systematic factors\n",
    "\n",
    "    # Now we get the beta, gamma (PDGSD), PD, EAD and LGD\n",
    "    Beta = Portfolio[[col for col in list(Portfolio) if col.startswith('beta')]].values\n",
    "    gamma = Portfolio['gamma'].values\n",
    "    PD = Portfolio['PD'].values\n",
    "    EAD = Portfolio['EAD'].values\n",
    "    LGD = Portfolio['LGD'].values\n",
    "\n",
    "    df_port = Portfolio[['SOV_ID','PD','EAD','LGD']]\n",
    "\n",
    "    # Analytical Expected Loss\n",
    "    EL_an = np.sum(PD*EAD*LGD)\n",
    "\n",
    "    # Calibrate default thresholds with PDs\n",
    "    d = norm.ppf(PD)\n",
    "    # perform a Cholesky factorisation to sample normal distributed\n",
    "    # numbers with covariaton matrix Omega\n",
    "    L = np.linalg.cholesky(Omega)\n",
    "\n",
    "    np.random.seed(10)\n",
    "    # generate independent normals\n",
    "    Z = np.random.standard_normal((p, N))\n",
    "\n",
    "    # convert independent unit normals to correlated\n",
    "    F = np.dot(L, Z)\n",
    "\n",
    "    # idiosyncratic loading s.t. the returns are standard normal\n",
    "    id_load = np.diagonal(np.sqrt(1-np.dot(np.dot(Beta,Omega),Beta.T)))\n",
    "    epsilon = np.random.standard_normal((N, m))\n",
    "    # Put everything together to get the returns\n",
    "    X = np.dot(Beta,F) + (id_load*epsilon).T\n",
    "    X_df = pd.DataFrame(np.dot(Beta,F) + (id_load*epsilon).T)\n",
    "\n",
    "    # Calculate UL with no contagion\n",
    "    # construct default indicator\n",
    "    I = (((X.T-d)<0))\n",
    "    I_df = pd.DataFrame(I)\n",
    "    L = (EAD*LGD*I).T\n",
    "    \n",
    "    # print(np.mean(L,axis=1))\n",
    "    Loss=np.sum(L,axis=0)\n",
    "\n",
    "    # Calculate UL with contagion\n",
    "\n",
    "    SOV_ID = Portfolio['SOV_ID'].values\n",
    "    SOV_LINK = Portfolio['SOV_LINK'].values\n",
    "\n",
    "    df_d = pd.DataFrame(np.zeros((m,3)), columns = ['SOV_ID','Dsd','Dnsd'])\n",
    "    df_d['SOV_ID']=SOV_ID\n",
    "\n",
    "    PDs = df_port[df_port['SOV_ID']==1]['PD'].values[0]\n",
    "\n",
    "    Dsd = np.zeros(m)\n",
    "    Dnsd = np.zeros(m)\n",
    "\n",
    "    # With contagion\n",
    "    for i in range(0,m):\n",
    "        if SOV_ID[i] != 0:\n",
    "            Dsd[i] = d[i]\n",
    "            Dnsd[i] = d[i]\n",
    "        else:\n",
    "            sov_ind = np.nonzero(SOV_ID == SOV_LINK[i])[0][0]\n",
    "            PDs = PD[sov_ind]\n",
    "            corr = np.dot(np.dot((Beta[i]).T,Omega),(Beta[sov_ind]))\n",
    "\n",
    "            Fsd = lambda x: mvn.mvndst([-100, -100],\\\n",
    "                [x, norm.ppf(PDs)],[0,0],corr)[1] / PDs - gamma[i]\n",
    "            Dsd[i] = fsolve(Fsd, norm.ppf(gamma[i])) # is there a better initial guess?\n",
    "            Fnsd = lambda x: mvn.mvndst([-100, norm.ppf(PDs)],\\\n",
    "                [x, 100],[0,1],corr)[1] - PD[i] + gamma[i]*PDs\n",
    "            Dnsd[i] = fsolve(Fnsd, norm.ppf(PD[i])) # is there a better initial guess?\n",
    "            if Dsd[i]< d[i] or PD[i]<PD[sov_ind]:\n",
    "                Dsd[i] = d[i]\n",
    "                Dnsd[i] = d[i]\n",
    "\n",
    "    df_d['Dsd'] = Dsd\n",
    "    df_d['Dnsd'] = Dnsd\n",
    "\n",
    "    # Thresholds\n",
    "    D = np.array([Dnsd]*N).T\n",
    "    D_df = pd.concat([df_d['Dnsd']]*N,axis = 1)\n",
    "    D_df.columns = range(N)\n",
    "\n",
    "    X2 = X_df.transpose()\n",
    "    D2 = D_df.transpose()\n",
    "\n",
    "    sov_ind = df_d[df_d['SOV_ID']==1].index[0]\n",
    "\n",
    "    X_SD = X2[X2[sov_ind]<df_d.loc[sov_ind, 'Dsd']].copy()\n",
    "\n",
    "    X_NSD = X2.drop(X_SD.index, axis = 0)\n",
    "\n",
    "\n",
    "    I_SD = X_SD.lt(df_d['Dsd'], axis = 1)\n",
    "    I_NSD = X_NSD.lt(df_d['Dnsd'],axis = 1)\n",
    "\n",
    "    I_c = pd.concat([I_SD,I_NSD], axis = 0)\n",
    "\n",
    "    I_aux = np.array(I_c)\n",
    "\n",
    "    L = (EAD * LGD * I_aux)\n",
    "\n",
    "    Loss_c = np.sum(L,axis=1)\n",
    "\n",
    "    EL = np.mean(Loss)\n",
    "    # Arithmetic mean of Loss\n",
    "    EL_c = np.mean(Loss_c)\n",
    "\n",
    "    # UL_98 = np.percentile(Loss, 98)\n",
    "    UL_99 = np.percentile(Loss, 99)\n",
    "    UL_995 = np.percentile(Loss, 99.5)\n",
    "    UL_999 = np.percentile(Loss, 99.9)\n",
    "    UL_9999 = np.percentile(Loss, 99.99)\n",
    "\n",
    "    # UL_98_c = np.percentile(Loss_c, 98)\n",
    "    UL_99_c = np.percentile(Loss_c, 99)\n",
    "    UL_995_c = np.percentile(Loss_c, 99.5)\n",
    "    UL_999_c = np.percentile(Loss_c, 99.9)\n",
    "    UL_9999_c = np.percentile(Loss_c, 99.99)\n",
    "\n",
    "\n",
    "    UL = np.array([ UL_99, UL_995, UL_999, UL_9999])\n",
    "    UL_c = np.array([ UL_99_c, UL_995_c, UL_999_c, UL_9999_c])\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    print(end-start)\n",
    "\n",
    "    return UL, UL_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Javier\\Documents\\MEGA\\Universitattt\\Master\\Thesis\\Results\\Capital_periods'\n",
    "folder1 = r'results_delay_'+str(delay)+'_per_'+str(per1)\n",
    "folder2 = r'results_delay_'+str(delay)+'_per_'+str(per2)\n",
    "\n",
    "no_con_1 = pd.read_csv(os.path.join(path,folder1,\\\n",
    "    'No_contagion_'+str(delay)+'_'+str(per1)+'.csv'),index_col = 0)\n",
    "con_1 = pd.read_csv(os.path.join(path,folder1,\\\n",
    "    'Contagion_'+str(delay)+'_'+str(per1)+'.csv'),index_col = 0)\n",
    "no_con_2 = pd.read_csv(os.path.join(path,folder2,\\\n",
    "    'No_contagion_'+str(delay)+'_'+str(per2)+'.csv'),index_col = 0)\n",
    "con_2 = pd.read_csv(os.path.join(path,folder2,\\\n",
    "    'Contagion_'+str(delay)+'_'+str(per2)+'.csv'),index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_perc = lambda x: \"{0:.2f}\".format(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = (con_2-con_1)/con_1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in comp.columns:\n",
    "    for i in comp.index:\n",
    "        comp.loc[i,c] = to_perc(comp.loc[i,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Javier\\Documents\\MEGA\\Universitattt\\Master\\Thesis\\Results\\Probabilities'\n",
    "files_1 = sorted([f for f in os.listdir(path) if \\\n",
    "\t\tf.endswith('_delay_'+str(delay)+'_per_'+str(per1))])\n",
    "files_2 = sorted([f for f in os.listdir(path) if \\\n",
    "\t\tf.endswith('_delay_'+str(delay)+'_per_'+str(per2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time_10_delay_1_per_0', 'time_15_delay_1_per_0', 'time_20_delay_1_per_0']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time_10_delay_1_per_5', 'time_15_delay_1_per_5', 'time_20_delay_1_per_5']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
